{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Quick overview of Python, Jupyter, and some of their features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through the basics features of Python and Jupyter in order to plot, create datasets, use dataframes, and do fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have never used Jupyter: Jupyter notebooks are a very cool and practical way of doing \"simple analysis\" and teaching. It allows an interactive framework to use Python, and at the same time allows the insertion fo comments and descriptions like the one you are reading now.\n",
    "\n",
    "This kind of cell is known as a MARKDOWN cell. Apart from this kind of cell, there is the more useful one for analysis: the CODE cell, like the following in which we import the necessary libraries of Python that we will use today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib                     # as the name says is a library for plotting in Python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np                    # library of numerical python functions\n",
    "import pandas as pd                   # library for data handling and data formats\n",
    "import scipy.stats as ss              # library of statistical methods\n",
    "from scipy.optimize import curve_fit  # function for fitting\n",
    "import uproot3                         # library to read .root files \"whithout ROOT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Creating histograms and filling them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we give a quick look to histograms and binning.\n",
    "There are two main methods you can use in python to deal with a histogram (i.e. a binned series):\n",
    " * `plt.hist(series, bins)`: you use this method in case you want to plot directly the histogram. This methods returns `n`, the values of the histogram bins, `bins`, the edges of the bins and `patches`, silent list of individual patches used to create the histogram or list of such list if multiple input datasets (patches are almost never used).\n",
    " * `np.hisogram(series, bins)`: compute the histogram of a set of data. This method returns `hist`, the values of the histogram and `bin_edges`, the bin edges `(length(hist)+1)`.\n",
    "\n",
    "Let's create some dummy data.\n",
    "\n",
    "We start by creating an array of $10^5$ random numbers, generated according to a `crystallball distribution`. Then we create two histograms using both `plt.hist()` and `np.histogram()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the values distributed as a gaussian\n",
    "beta, m = 0.4, 5 \n",
    "cb_vals = ss.crystalball.rvs(beta, m, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning = np.linspace(-20, 5, 200)\n",
    "n, bins, patches = plt.hist(cb_vals, binning, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `plt.hist` automatically plots the histogram of the series we are providing to this method. Also note that it is preferable to assing `plt.hist()` to a variable, otherwise you also get the printout of all the contents in each bin. For example, try to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(cb_vals, binning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, if we do not directly need to see the histogram, it is better to use `np.hisogram` (especially if you do not want the plots to appear and make your notebook longer and longer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bin_edges = np.histogram(cb_vals, bins,  density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth mentioning that the bins edges returned by the two methods have length larger than the values of the histogram. Therefore, we need to remove the overflow bin when we want to plot `bins` vs `n` (or `bin_edges` vs `hist`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "t = plt.hist(cb_vals, binning,  density=True)\n",
    "plt.plot(bins[:-1], n, 'ko', label = 'points from plt.hist()')\n",
    "plt.plot(bin_edges[:-1], hist, 'r*', label = 'points from np.histogram()')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, the bins edges above do not correspond to the bins centers. This is why in the plot above, the points are not aligned with the histogram. Let's define the bin centers and then use these points to fit the above distribution with a gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = (bins[:-1] + bins[1:])/2\n",
    "bin_edges = (bin_edges[:-1] + bin_edges[1:])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.title('Aligned bin edges')\n",
    "t = plt.hist(cb_vals, binning,  density=True)\n",
    "plt.plot(bins, n, 'ko', label = 'points from plt.hist()')\n",
    "plt.plot(bin_edges, hist, 'r*', label = 'points from np.histogram()')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Fitting histograms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having generated a distribution we can fit it with a user-defined function, using the `curve_fit` method with a user defined function. Let's start by defining the `gaussian` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, A, sigma, mu):\n",
    "    return A/(sigma * np.sqrt(2 * np.pi))*np.exp( - (x - mu)**2 / (2 * sigma**2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the distributuin with a gaussian. \n",
    "\n",
    "`curve_fit` expects 3 arguments:\n",
    " * The fit function, in our case `gaussian`;\n",
    " * The `x` and `y` arrays of data;\n",
    "\n",
    "For more options (such as setting initial guess for the free parameters, including uncertainties to the fit and boundaries on the fit free parameters) you can have a look at the [`curve_fit` documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(gaussian, bins, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit results are:\n",
    " * `popt`: Optimal values for the parameters so that the sum of the squared residuals of `f(xdata, *popt) - ydata` is minimized;\n",
    " * `pcov`: The estimated covariance of popt. The diagonals provide the variance of the parameter estimate. To compute one standard deviation errors on the parameters use `perr = np.sqrt(np.diag(pcov))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.title('Fit of the distribution')\n",
    "plt.plot(bins, n, 'ko', label = 'points from plt.hist()')\n",
    "plt.plot(bin_edges, hist, 'r*', label = 'points from np.histogram()')\n",
    "plt.plot(binning, gaussian(binning, *popt), '-', label = 'Gaussian fit')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would expect the `gussian` does not fit well the daat. Therefore we we have to use the `crystalball.fit` fitting. As input it takes:\n",
    "* the data to be fitted (`cb_vals`)\n",
    "* the starting guess of the parameters to fit: in our case we start guessing the normalization with `scale=1`\n",
    "Complete documentation can be found [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.crystalball.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_fit, m_fit, offset_fit, norm_fit = ss.crystalball.fit(cb_vals, scale=1)\n",
    "print(\"MLE beta =\", beta_fit, \"; MLE m =\", m_fit, \"; MLE offset =\", offset_fit, \"; MLE norm =\", norm_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the fitting is the Maximum Likelihood Estimate (MLE) of the parameters of the `crystallball` distribution. Now we can plot it to visually confront the result of teh fit with the original distribution we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.title('Fit of the distribution')\n",
    "plt.plot(bins, n, 'ko', label = 'points from plt.hist()')\n",
    "plt.plot(bin_edges, hist, 'r*', label = 'points from np.histogram()')\n",
    "plt.plot(binning, gaussian(binning, *popt), '-', label = 'Gaussian fit')\n",
    "rv = ss.crystalball(beta_fit, m_fit, offset_fit, norm_fit)\n",
    "plt.plot(binning, rv.pdf(binning), '-', label = 'Crystalball fit')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Dataset handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's import the dataset we are going to use.\n",
    "Normally HEP data come in the `.root` format, which was born to be directly usable with the [`ROOT` framework](https://root.cern.ch/). We are going to open it into a `pandas` data frame, using [`uproot`](https://github.com/scikit-hep/uproot) to import the ROOT `TTree`.\n",
    "We also have to define a `key`, which is the name of the `TTree` we want to open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree = uproot3.open('/data_CMS/cms/motta/HHLegacy_SKIMS/SKIMS2018/TREX/SKIM_GGHH_NLO_cHHH1_xs.root')['HTauTauTree']\n",
    "df = tree.pandas.df([\"dau1_pt\", \"dau1_eta\", \"dau1_phi\", \"dau1_flav\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2.1 Selecting events is Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have in mind the structure of the data frame, we can get some pandas basics knowledge. Let's say we want to look at all the entries in our `df` where the `dau1_pt` is larger than 50GeV. In pandas this can be quickly done with a `query` on the `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we ask for head() because \n",
    "## df[sel] is still a df\n",
    "sel = df['dau1_pt'] > 50\n",
    "df[sel].head() # or df.query('rechit_energy > 20.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have learnt how to apply a boolean selection to a data frame. The `sel` object is a pandas Series made of `True` and `False` entries according to whether the selection (`df['rechit_energy'] > 20`) is satisfied or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine multiple selections and create a new df starting from the original one, but with a selection applied on data. Something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We define some selections\n",
    "sel_energy = df['dau1_pt'] > 50\n",
    "sel_eta = df['dau1_eta'] < 2.3\n",
    "\n",
    "## We combine the selection together.\n",
    "## Note that python uses the single & (or | ).\n",
    "sel_tot = sel_energy & sel_eta\n",
    "\n",
    "## We create a new df with only the events\n",
    "## we are interested on.\n",
    "df_cut = df[sel_tot]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the difference in length between these two data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2.1 Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even group items, perform calculations on some aggregated property, then plot values, just to get an idea of larger scale trends. This property of grouping the elements of the data frame together can turn out to be very useful. In `pandas` you can `groupby()` a data frame by items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby('dau1_flav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gb is an object from which we can get properties on aggregrated items of df\n",
    "gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's say we want to check the min/max value of a variable per event. \n",
    "## Here an example for the reconstructed energy:\n",
    "\n",
    "gb.agg({'dau1_pt':'min'}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb.agg({'dau1_pt':'max'}).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
